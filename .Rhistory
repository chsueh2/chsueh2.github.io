proc_timer <- proc.time()
# train model
fit <- train(
form = formula,
data = df_train,
method = method,
preProcess = c("center", "scale"),
trControl = trControl,
tuneGrid = tuneGrid, ...)
# timer - report time used
print(proc.time() - proc_timer)
# print the best tune if there is a tuning parameter
if(is.null(tuneGrid)){
print("No tuning parameter")
} else {
# print the best tune
print("The best tune is found with:")
print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
if(plot) plot_modelinfo(fit)
}
# make prediction on test set
pred <- predict(fit, newdata = df_test) %>% as_factor()
print(pred)
# return performance metric or confusion matrix depending on response type
if(is.numeric(pred)){
print("rmse")
# numeric response
performance <- postResample(pred, obs = df_test[,1] %>% as_vector())
# print performance metrics
print("Performance metrics:")
print(performance)
# return the performance metric
return(performance)
} else if(is.factor(pred)){
print("accuracy")
# categorical response
cfm <- confusionMatrix(pred, df_test[,1] %>% as_vector())
# print confusion matrix and accuracy
print("Confusion table:")
print(cfm$table)
print(glue("Accuracy = {cfm$overall['Accuracy']}"))
# return the confusion matrix
return(cfm)
}
}
# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
# get model info
model <- fit$modelInfo$label
parameter <- fit$modelInfo$parameters$parameter
description <- fit$modelInfo$parameters$label
# plot parameter vs metrics
p <- fit$results %>%
rename_at(1, ~"x") %>%
pivot_longer(cols = -1, names_to = "Metric") %>%
ggplot(aes(x, value, color = Metric)) +
geom_point() +
geom_line() +
facet_grid(rows = vars(Metric), scales = "free_y") +
labs(
title = glue("{model}: Hyperparameter Tuning"),
x = glue("{parameter} ({description})")
)
print(p)
return(p)
}
knn_fit <- fit_model(
Class ~ ., df_train, df_test, method = "knn",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(k = 1:10),
plot = TRUE
)
# a wrapper function to train a model with train set and calculate the model performance on test set
fit_model <- function(
formula, df_train, df_test, method,
preProcess = c("center", "scale"),
trControl = trainControl(),
tuneGrid = NULL,
plot = FALSE, ... ){
# timer - start
proc_timer <- proc.time()
# train model
fit <- train(
form = formula,
data = df_train,
method = method,
preProcess = c("center", "scale"),
trControl = trControl,
tuneGrid = tuneGrid, ...)
# timer - report time used
print(proc.time() - proc_timer)
# print the best tune if there is a tuning parameter
if(is.null(tuneGrid)){
print("No tuning parameter")
} else {
# print the best tune
print("The best tune is found with:")
print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
if(plot) plot_modelinfo(fit)
}
# make prediction on test set
pred <- predict(fit, newdata = df_test) %>% as_factor()
print(pred)
# return performance metric or confusion matrix depending on response type
if(is.numeric(pred)){
print("rmse")
# numeric response
performance <- postResample(pred, obs = df_test[,1] %>% as_factor())
# print performance metrics
print("Performance metrics:")
print(performance)
# return the performance metric
return(performance)
} else if(is.factor(pred)){
print("accuracy")
# categorical response
cfm <- confusionMatrix(pred, df_test[,1] %>% as_vector())
# print confusion matrix and accuracy
print("Confusion table:")
print(cfm$table)
print(glue("Accuracy = {cfm$overall['Accuracy']}"))
# return the confusion matrix
return(cfm)
}
}
# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
# get model info
model <- fit$modelInfo$label
parameter <- fit$modelInfo$parameters$parameter
description <- fit$modelInfo$parameters$label
# plot parameter vs metrics
p <- fit$results %>%
rename_at(1, ~"x") %>%
pivot_longer(cols = -1, names_to = "Metric") %>%
ggplot(aes(x, value, color = Metric)) +
geom_point() +
geom_line() +
facet_grid(rows = vars(Metric), scales = "free_y") +
labs(
title = glue("{model}: Hyperparameter Tuning"),
x = glue("{parameter} ({description})")
)
print(p)
return(p)
}
knn_fit <- fit_model(
Class ~ ., df_train, df_test, method = "knn",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(k = 1:10),
plot = TRUE
)
# a wrapper function to train a model with train set and calculate the model performance on test set
fit_model <- function(
formula, df_train, df_test, method,
preProcess = c("center", "scale"),
trControl = trainControl(),
tuneGrid = NULL,
plot = FALSE, ... ){
# timer - start
proc_timer <- proc.time()
# train model
fit <- train(
form = formula,
data = df_train,
method = method,
preProcess = c("center", "scale"),
trControl = trControl,
tuneGrid = tuneGrid, ...)
# timer - report time used
print(proc.time() - proc_timer)
# print the best tune if there is a tuning parameter
if(is.null(tuneGrid)){
print("No tuning parameter")
} else {
# print the best tune
print("The best tune is found with:")
print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
if(plot) plot_modelinfo(fit)
}
# make prediction on test set
pred <- predict(fit, newdata = df_test) %>% as_factor()
print(pred)
# return performance metric or confusion matrix depending on response type
if(is.numeric(pred)){
print("rmse")
# numeric response
performance <- postResample(pred, obs = df_test[,1] %>% as_vector()())
# print performance metrics
print("Performance metrics:")
print(performance)
# return the performance metric
return(performance)
} else if(is.factor(pred)){
print("accuracy")
# categorical response
cfm <- confusionMatrix(pred, df_test[,1] %>% as_factor())
# print confusion matrix and accuracy
print("Confusion table:")
print(cfm$table)
print(glue("Accuracy = {cfm$overall['Accuracy']}"))
# return the confusion matrix
return(cfm)
}
}
# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
# get model info
model <- fit$modelInfo$label
parameter <- fit$modelInfo$parameters$parameter
description <- fit$modelInfo$parameters$label
# plot parameter vs metrics
p <- fit$results %>%
rename_at(1, ~"x") %>%
pivot_longer(cols = -1, names_to = "Metric") %>%
ggplot(aes(x, value, color = Metric)) +
geom_point() +
geom_line() +
facet_grid(rows = vars(Metric), scales = "free_y") +
labs(
title = glue("{model}: Hyperparameter Tuning"),
x = glue("{parameter} ({description})")
)
print(p)
return(p)
}
knn_fit <- fit_model(
Class ~ ., df_train, df_test, method = "knn",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(k = 1:10),
plot = TRUE
)
df_test[, 1]
df_test[, 1, drop = T]
df_test[, 1, drop = F]
data("GermanCredit")
df <- GermanCredit %>%
mutate(Class = factor(Class)) %>%
relocate(Class)
# split train/test sets
set.seed(90)
trainIndex <- createDataPartition(df$Class, p = 0.7, list = FALSE)
df_train <- df[trainIndex, ]
df_test <- df[-trainIndex, ]
names(df)
# a wrapper function to train a model with train set and calculate the model performance on test set
fit_model <- function(
formula, df_train, df_test, method,
preProcess = c("center", "scale"),
trControl = trainControl(),
tuneGrid = NULL,
plot = FALSE, ... ){
# timer - start
proc_timer <- proc.time()
# train model
fit <- train(
form = formula,
data = df_train,
method = method,
preProcess = c("center", "scale"),
trControl = trControl,
tuneGrid = tuneGrid, ...)
# timer - report time used
print(proc.time() - proc_timer)
# print the best tune if there is a tuning parameter
if(is.null(tuneGrid)){
print("No tuning parameter")
} else {
# print the best tune
print("The best tune is found with:")
print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
if(plot) plot_modelinfo(fit)
}
# make prediction on test set
pred <- predict(fit, newdata = df_test)
print(pred)
# return performance metric or confusion matrix depending on response type
if(is.numeric(pred)){
print("rmse")
# numeric response
performance <- postResample(pred, obs = df_test[,1] %>% as_vector()())
# print performance metrics
print("Performance metrics:")
print(performance)
# return the performance metric
return(performance)
} else if(is.factor(pred)){
print("accuracy")
# categorical response
cfm <- confusionMatrix(df_test[, 1], pred)
# print confusion matrix and accuracy
print("Confusion table:")
print(cfm$table)
print(glue("Accuracy = {cfm$overall['Accuracy']}"))
# return the confusion matrix
return(cfm)
}
}
# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
# get model info
model <- fit$modelInfo$label
parameter <- fit$modelInfo$parameters$parameter
description <- fit$modelInfo$parameters$label
# plot parameter vs metrics
p <- fit$results %>%
rename_at(1, ~"x") %>%
pivot_longer(cols = -1, names_to = "Metric") %>%
ggplot(aes(x, value, color = Metric)) +
geom_point() +
geom_line() +
facet_grid(rows = vars(Metric), scales = "free_y") +
labs(
title = glue("{model}: Hyperparameter Tuning"),
x = glue("{parameter} ({description})")
)
print(p)
return(p)
}
knn_fit <- fit_model(
Class ~ ., df_train, df_test, method = "knn",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(k = 1:10),
plot = TRUE
)
table(df$Personal.Female.Single)
names(df)
table(df$Personal.Female.NotSingle)
data("GermanCredit")
df <- GermanCredit %>%
mutate(Class = factor(Class)) %>%
relocate(Class) %>%
# remove this variable since it only contains one level
select(-Personal.Female.Single)
# split train/test sets
set.seed(90)
trainIndex <- createDataPartition(df$Class, p = 0.7, list = FALSE)
df_train <- df[trainIndex, ]
df_test <- df[-trainIndex, ]
names(df)
# a wrapper function to train a model with train set and calculate the model performance on test set
fit_model <- function(
formula, df_train, df_test, method,
preProcess = c("center", "scale"),
trControl = trainControl(),
tuneGrid = NULL,
plot = FALSE, ... ){
# timer - start
proc_timer <- proc.time()
# train model
fit <- train(
form = formula,
data = df_train,
method = method,
preProcess = c("center", "scale"),
trControl = trControl,
tuneGrid = tuneGrid, ...)
# timer - report time used
print(proc.time() - proc_timer)
# print the best tune if there is a tuning parameter
if(is.null(tuneGrid)){
print("No tuning parameter")
} else {
# print the best tune
print("The best tune is found with:")
print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
if(plot) plot_modelinfo(fit)
}
# make prediction on test set
pred <- predict(fit, newdata = df_test)
# return performance metric or confusion matrix depending on response type
if(is.numeric(pred)){
print("rmse")
# numeric response
performance <- postResample(pred, obs = df_test[, 1])
# print performance metrics
print("Performance metrics:")
print(performance)
# return the performance metric
return(performance)
} else if(is.factor(pred)){
# categorical response
cfm <- confusionMatrix(df_test[, 1], pred)
# print confusion matrix and accuracy
print("Confusion table:")
print(cfm$table)
print(glue("Accuracy = {cfm$overall['Accuracy']}"))
# return the confusion matrix
return(cfm)
}
}
# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
# get model info
model <- fit$modelInfo$label
parameter <- fit$modelInfo$parameters$parameter
description <- fit$modelInfo$parameters$label
# plot parameter vs metrics
p <- fit$results %>%
rename_at(1, ~"x") %>%
pivot_longer(cols = -1, names_to = "Metric") %>%
ggplot(aes(x, value, color = Metric)) +
geom_point() +
geom_line() +
facet_grid(rows = vars(Metric), scales = "free_y") +
labs(
title = glue("{model}: Hyperparameter Tuning"),
x = glue("{parameter} ({description})")
)
print(p)
return(p)
}
# a wrapper function to train a model with train set and calculate the model performance on test set
fit_model <- function(
formula, df_train, df_test, method,
preProcess = c("center", "scale"),
trControl = trainControl(),
tuneGrid = NULL,
plot = TRUE, ... ){
# timer - start
proc_timer <- proc.time()
# train model
fit <- train(
form = formula,
data = df_train,
method = method,
preProcess = c("center", "scale"),
trControl = trControl,
tuneGrid = tuneGrid, ...)
# timer - report time used
print(proc.time() - proc_timer)
# print the best tune if there is a tuning parameter
if(is.null(tuneGrid)){
print("No tuning parameter")
} else {
# print the best tune
print("The best tune is found with:")
print(glue("\t{names(fit$bestTune)} = {fit$bestTune[1,]}"))
if(plot) plot_modelinfo(fit)
}
# make prediction on test set
pred <- predict(fit, newdata = df_test)
# return performance metric or confusion matrix depending on response type
if(is.numeric(pred)){
print("rmse")
# numeric response
performance <- postResample(pred, obs = df_test[, 1])
# print performance metrics
print("Performance metrics:")
print(performance)
# return the performance metric
return(performance)
} else if(is.factor(pred)){
# categorical response
cfm <- confusionMatrix(df_test[, 1], pred)
# print confusion matrix and accuracy
print("Confusion table:")
print(cfm$table)
print(glue("Accuracy = {cfm$overall['Accuracy']}"))
# return the confusion matrix
return(cfm)
}
}
# a helper function to plot the metric vs. the tuning parameter
plot_modelinfo <- function(fit){
# get model info
model <- fit$modelInfo$label
parameter <- fit$modelInfo$parameters$parameter
description <- fit$modelInfo$parameters$label
# plot parameter vs metrics
p <- fit$results %>%
rename_at(1, ~"x") %>%
pivot_longer(cols = -1, names_to = "Metric") %>%
ggplot(aes(x, value, color = Metric)) +
geom_point() +
geom_line() +
facet_grid(rows = vars(Metric), scales = "free_y") +
labs(
title = glue("{model}: Hyperparameter Tuning"),
x = glue("{parameter} ({description})")
)
print(p)
return(p)
}
knn_fit <- fit_model(
Class ~ ., df_train, df_test, method = "knn",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(k = 1:10)
)
table(df$Purpose.Vacation)
data("GermanCredit")
df <- GermanCredit %>%
mutate(Class = factor(Class)) %>%
relocate(Class) %>%
# remove these variables since it only contains one level
select(-Personal.Female.Single, -Purpose.Vacation)
# split train/test sets
set.seed(90)
trainIndex <- createDataPartition(df$Class, p = 0.7, list = FALSE)
df_train <- df[trainIndex, ]
df_test <- df[-trainIndex, ]
names(df)
knn_fit <- fit_model(
Class ~ ., df_train, df_test, method = "knn",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(k = 1:10)
)
rf_fit <- fit_model(
Class ~ ., df_train, df_test, method = "rf",
trControl = trainControl(method = "repeatedcv", number = 3, repeats = 1),
tuneGrid = expand.grid(mtry = 1:10)
)
